# Deep Learning Track
The Deep Learning track focuses on IR tasks where a large training set is available, allowing us to compare a variety of retrieval approaches including deep neural networks and strong non-neural approaches, to see what works best in a large-data regime.
Anticipated timeline: Results due in early August
Track coordinators:
Nick Craswell, Microsoft
Bhaskar Mitra, Microsoft Research
Emine Yilmaz, University College London
Daniel Campos, University of Illinois at Urbana-Champaign
Jimmy Lin, University of Waterloo
Track Web Page: https://microsoft.github.io/msmarco/TREC-Deep-Learning

<hr>

Here are the Summary for the TREC 2023 Deep Learning Track (summarized from TRAC website above):

**Timetable** (tentative):
- July 10th: Test queries released
- July 31st: Deadline for submitting runs for passage and document ranking tasks

**Registration**:
To participate in TREC, pre-registration is required. You can pre-register at the following website: [TREC 2023 Deep Learning Track Pre-Registration](https://ir.nist.gov/trecsubmit.open/application.html)

**Introduction**:
The TREC Deep Learning Track focuses on information retrieval in a large training data regime, where the number of training queries with at least one positive label is in the tens of thousands or more. This corresponds to real-world scenarios such as training based on click logs and labels from shallow pools. The track aims to provide large-scale datasets to TREC and evaluate rankers for passage ranking and document ranking tasks using deep learning and other machine learning-based methods.

**Deep Learning Track Tasks**:
The 2023 Deep Learning Track will have the following tasks:
1. Passage Ranking Task: Participants are expected to rank passages from the full collection based on their likelihood of containing an answer to the question. There are two subtasks for this: Full ranking (retrieval) and top-100 reranking.
2. Document Ranking Task: Participants are expected to rank documents based on their likelihood of containing a passage relevant to the question. Similar to the passage ranking task, there are two subtasks: Full ranking (retrieval) and top-100 reranking.

**Datasets**:
Both the passage ranking task and document ranking task use a large human-generated set of training labels from the MS MARCO dataset. The test queries are the same for both tasks. Participants are encouraged to use transfer learning methods and can use external corpora for large-scale language model pretraining. The passage ranking dataset and document ranking dataset have specific formats and include training queries, test queries, and relevant documents/passages.

**Use of External Information**:
Participants are allowed to use external information while developing their runs, but they must fill in a form listing the resources used during submission. The use of ORCAS data is prohibited for this year, but participants are permitted to use any data listed in the guidelines.

**Submission, Evaluation, and Judging**:
Participants are allowed to submit up to three official runs for each subtask that will be evaluated by NIST. They can also submit up to five additional runs for evaluation, but those will not be included in the pooling/judging process. The format for submission is similar to the one used in most TREC submissions, with specific columns for topic number, passage/document identifier, rank, score, and run ID. Evaluation will be conducted using the provided test queries, and different approaches will be used for constructing test collections for passage ranking and document ranking tasks.

**Additional Resources**:
The TREC 2023 Deep Learning Track provides additional resources, including a segmented document collection and an augmented passage collection.

**TREC 2023 Deep Learning Track**

**Rough Plan**:

1. **How/When to Start**: To participate in the TREC 2023 Deep Learning Track, participants should pre-register on the official TREC website. After pre-registration, participants will gain access to the test queries released on July 10th. This will allow them to start developing their ranking models and strategies based on the provided datasets and guidelines.

2. **Data Set**: The track utilizes the MS MARCO dataset for training and evaluation. It includes a passage ranking dataset and a document ranking dataset. Both datasets contain training queries, test queries, and relevant passages/documents. The details, such as file sizes and formats, can be found in the provided guidelines.

3. **Topics**: The official test queries, to be released on July 10th, will be used for both the passage ranking and document ranking tasks. Participants will receive the same set of test queries for evaluating their ranking models.

4. **Models**: Participants are encouraged to explore deep learning and other machine learning-based methods suitable for information retrieval in large-scale data regimes. They can experiment with transfer learning techniques and incorporate external resources, such as pre-trained language models (e.g., BERT) or additional datasets, to enhance their models' performance.

5. **Due Date**: The deadline for submitting runs for the passage and document ranking tasks is July 31st. To be included in the evaluation and judging process, participants should ensure they submit their runs before this deadline.

6. **Paper**: While the guidelines do not explicitly mention a separate paper submission, participants often present their approaches, methodologies, and experimental results in research papers. This allows them to share their findings and contributions with the community. Participants should follow any additional instructions or announcements from the organizers regarding paper submissions.

# Professor's Instructions:
## Wed, Jul 19, 11:38 PM

Hi all,

Thanks very much for your quick response. Here are the stats:

- Clinical Trials: Sean
- NeuCLIR: Chang
- Product Search: Chang, Aary
- Crisis Facts: Aary, Monty, Reza
- <b> Deep Learning: Monty, Reza </b>
- iKAT: Aritra

So our next move is to have a plan. Can you go visit the website of each task and then 
- ### have a rough plan, such as how/when to start,
- ### data set,
- ### topics,
- ### models,
- ### due date,
- ### paper?

Our next meeting, by group, will talk about the feasibility of your plan. You are good to shift yourself to another project during this stage and feel comfortable to join one or two projects. 

By the way, do not forget to tell me who the leader is in each group. Best,

# Tue, Jul 18, 1:39 AM

Hi all,

I hope all of you have had a wonderful summer so far, and sorry for the late email. I would like to invite all of you to participate in some summer projects. 

Here are the potential project(s) you might be interested in. The website is: https://trec.nist.gov/pubs/call2023.html. Please choose 1 or 2 projects from the list and check their websites for more information.

We will make some groups based on the project (each track is a project). Besides, I wish Aritra, Chang and Sean could be the leader at this time since you have worked on TREC 2022 last year. So our member teammates can join your teams and work together. 

The next action will be:
Reply to me your interests as soon as possible. I will suggest this Friday.
Meet the groups individually.
### Start to work such as literature review, conducting experiments using previous year's data, targeting the deadline, etc.
Please let me know if you have any questions. I will forward you the password of TREC 2023 to get into other categories.

Best,

Vivian

# Tue, Jul 18, 1:39 AM

Welcome to TREC 2023.

 

You have received this message as a result of being on the TREC 2023 participants' mailing list.  You were put on the participants list either because you signed up to participate in TREC 2023, or because you are a TREC 2023 program committee member or track organizer.  If your organization decides not to participate in TREC 2023, please send mail to trec@nist.gov, and we will remove you from the list.

 

This very long message serves as an introduction to TREC.  Groups that have participated in earlier TRECs will need to pay particular attention to Section 1, What's New in TREC 2023.  New participants should read the entire message carefully at some point to understand the TREC process.  You should save this message somewhere where you will be able to refer to it easily since you'll need to refer to different parts of the message at various times in the next 10 months.

 

 

Section 1 -- What's New in TREC 2023

 

  1.  The active participants section of the Web site has a new login/password sequence (case is significant).

     

 

The web site is used as the main communication vehicle for TREC participants.  All data to be downloaded from NIST will be on the Web site, track guidelines will be posted there, and email sent to this mailing list will be archived on the site.  If you think you may have had email problems, please be sure to check the email archive to see if you missed any TREC email.

 

  2.  Four tracks from last year are continuing, and there are four new tracks in TREC 2023: Authoring Tools for Multimedia Content (AToMiC, new), Clinical Trials, CrisisFACTs, Deep Learning, Interactive Knowledge Assistance (IKAT, new), NeuCLIR, Product Search (new), and Tip-of-the-Tongue Search (new).

 

      Tracks are in different stages of finalizing their task definitions for TREC 2023.  You still have time to comment on the task definitions for the tracks.  Each track has a Slack channel, web page, and/or mailing list dedicated to it: it is important to join the channels and/or mailing lists of all of the tracks you might possibly participate in so you can have a voice in defining the task, receive the drafts of the track guidelines, and learn about issues, changes, etc. as they occur.  To join a mailing list, please follow the instructions given on the TREC tracks page; NIST does not subscribe you to a track mailing list even if your application noted an interest in the track.  If you were subscribed to a track's list in a previous TREC, you do not have to re-subscribe since track mailing lists are for anyone interested in the problem, rather than specifically for TREC participants, and thus carry over from year to year (assuming the mailing list name has stayed the same).

 

      The tracks page in the active participants' section of the TREC web site (https://trec.nist.gov/act_part/tracks2023.html) lists the track goals, contacts, guidelines, and other information about the track as we receive it from the track coordinators.  Right now, the page generally contains only contact information; it will be updated throughout TREC 2023.

 

  3. TREC also uses Slack channels for communication among active participants.  Slack is largely (but not entirely) replacing track mailing lists.  The TREC Slack workspace is at https://trectalk.slack.com and includes channels for general questions as well as a channel for each track.  You should have received an invitation to join the TREC Slack workspace when your application to participate in TREC 2023 was processed.  For assistance with Slack, contact angela.ellis@nist.gov.

 

  4.  Each track will have its own deadline for submitting results, which will be given in the final guidelines.  Some tracks may have very early deadlines in comparison to other years.  If you have particular concerns about deadlines you are welcome to express them (early) on the track mailing list, but please be aware that TREC may not be able to accommodate all requests.

 

  5.  When you registered to participate in TREC you selected a team name.  Please include this team name in all TREC correspondence with NIST.  If you are returning a form back to NIST, include the team name somewhere on the form and/or cover email. (Do not use physical mail or fax anything to NIST at this time because NIST staff are not present full-time at NIST.)  You will also need to use the team name when you submit your results.  Including the team name in all correspondence will help us to serve you better.  TREC had 133 teams sign up to participate in TREC 2022, and there are always instances of multiple teams from within the same organization.  We don't always recognize what specific team is intended when we have just a person's name.

 

      Your team name is also used as the unique identifier for your group in the data we maintain about participants, including runs submitted.  The use of the team name has evolved such that we now organize the list of runs submitted and the index of proceedings papers by team name.  If you submitted an application using a name that you would rather not use as a public ID, NOW is the time to make that change.  It is too complicated to change team IDs later in the TREC year.  To make a change, have the main TREC contact person send mail to trec@nist.gov giving the original team name and what you would like it changed to.

 

  6.  TREC has evolved a standard set of policies and guidelines that all participants are expected to follow.  The policies are in place to protect the desired pre-competitive nature of the conference and to encourage continued participation in the conference.  The main components of the policy are:

 

      * dissemination of TREC results is permitted only in accordance with the "Agreement Concerning Dissemination of TREC Results", which is found in the forms section of the active participants' section of the web site, specifically, at https://trec.nist.gov/act_part/forms/noads.pdf.  All TREC 2023 participants are required to sign and return this form (even if your group has signed one in the past).

 

        Sign the form and return a scanned copy of the signed form to trec@nist.gov, including the fact that it is a TREC 2023 dissemination form in the subject of the message and remembering to include your team name.

 

   !!!! You will not be able to submit results unless we have a signed form on file.

 

      * attendance at the TREC conference in November is limited to organizations that actually submit retrieval results.

 

      * if you do submit results, they may not be subsequently withdrawn (i.e., all results are archived on the web site and evaluated in the Appendix of the proceedings).

 

      * all groups that submit results are expected to write a paper for the proceedings that describes how the runs were produced (to the extent intellectual property concerns allow).  If you do not submit results, you may not contribute a paper to the proceedings.

 

     * the active participants' section contains a "guidelines" link that contains further details about producing retrieval results.  In general, we expect groups to do no training or customizing of their systems with respect to the current year's test data.

 

----------------------------------------------------------------------------

Section 2 -- Instructions

 

The following is meant to help in answering the various questions we have received from those new to TREC.  Please feel free to contact Ian Soboroff (ian.soboroff@nist.gov) if something is not clear.  It's much better to ask questions now rather than closer to the deadlines.

 

Email and Slack are the only forms of communication used in TREC.  NIST strives to keep the traffic on the main participants' mailing list to the minimum possible, but it is important that you actually read the messages posted to the list.  (No other messages on the list will be anywhere near as long as this one!)  Also, please make sure that everyone in your organization that will be involved with TREC has access to the messages.  To simplify our administrative overhead, we use exactly one mail address per group on the main participants' mailing list, but we encourage the use of internal reflectors or local mailing lists.  Send mail to trec@nist.gov if you'd like to change the current contact address for the TREC 2023 mailing list to a local mailing list address.  Note that all email sent to the list is archived in the active participants' section of the TREC web site under "Email Archive".

 

Having said that, please also make sure that you do NOT redistribute these messages outside your group.  A common mistake is to post the messages to a web site that is accessible to web search engine crawlers.  The whole point of having the participants' section be password protected is to exclude non-participants from the current year's TREC data, and that purpose is defeated if TREC email is publicly accessible.

 

The TREC web site, https://trec.nist.gov, plays an important role in TREC operations.  NIST tries to ensure that everything you need to know about the current TREC is available in the Active Participants' section of the site.  The TREC web site also contains a "Publications" section that contains electronic versions of the Proceedings of past TRECs and some other TREC-related presentations.  The "Overview" papers in the Proceedings provide many details about the TREC tasks for that year, and give a sense of the overall TREC process.

 

To participate in TREC, you must abide by the conditions set forth in the "Agreement Concerning Dissemination of TREC Results" as described in the TREC Statement on Product Testing and Advertising (https://trec.nist.gov/trec.disclaim.html).  The use of the TREC conference results (or the sponsors' names) in advertising is not only contrary to the spirit of TREC, but is likely to result in difficulties for the sponsoring agencies (which will bring a quick end to TREC).  If you feel that your organization cannot adhere to these guidelines, then you should withdraw from TREC now.  As stated above, you must submit a signed copy of the form to NIST to participate in TREC.

 

To attend the TREC workshop in November you must submit the retrieval runs for at least one task to NIST by the deadline given in the guidelines.  Every year some groups that originally intended to participate are unable to submit results for one reason or another.  We understand that this can happen, and there is no "penalty" other than not being able to attend the workshop.  Once the final deadline is passed, we will remove organizations that did not submit any results from the participants' mailing list.  All runs that are submitted to NIST will be published in the Appendix of the proceedings and archived in the results section of the web site.  You may not withdraw a run after it has been submitted to NIST.

 

Conference attendees have access to a "notebook" containing the drafts of participants' proceedings papers.  The actual proceedings are published after the conference.  The idea for having both a notebook version and a proceedings version of papers is that the notebook gives conference attendees something concrete during the meeting itself, while the proceedings give participants more time to write and the opportunity to incorporate cross-group discussion.  Every group that submits results is expected to write a paper describing how those results were produced.

 

Much of the data used in very early TRECs is available to you to use in developing your retrieval system.  See the "Data" section of the TREC web site for details about previously created test collections.  These test collections are made available as a convenience for you to use to develop your retrieval system.  It is important to check the track mailing list and guidelines to get information regarding what document collection will be used in a particular track in TREC 2023.

 

Relevance judgments in TREC are called "qrels" (from query-relevance files).  For traditional TREC test collections, judgments were made using a pooling technique.  For these collections, you can assume the judgments are "complete"---that enough results have been assembled and judged to assume that most relevant documents have been found.  For most later tracks judgments were made using different sampling techniques, and this assumption does not necessarily hold.

 

The Qrels format is:

            topicno  iteration  docno  relevancy

where

      * topicno is the topic number,

      * iteration is the feedback iteration (almost always zero and generally not used),

      * docno is the official document number that corresponds to the docno field in the documents, and relevancy is

      * an integer representing the judgment for that document.

For traditional collections, all documents not listed in the qrels are assumed irrelevant.  The difference between being marked as irrelevant in the qrels and not appearing in the qrels at all is that the former was looked at by a relevance assessor and judged irrelevant while the latter was never looked at.  The order of documents in the qrels file is not indicative of relevance or degree of relevance.  Most early qrels give only a binary indication of relevant (1) or non-relevant (0).  More recent tasks were frequently judged on a three-way scale using non-relevant (0),  relevant (1), and highly relevant (2).  Some tasks have used other scales; the track overview paper corresponding to that qrels set will describe the judgments.  Note that trec_eval, the evaluation package available from the Tools page on the web site, assumes binary judgments for most measures and by default treats any positive value as relevant.

 

 

----------------------------------------------------------------------------

 

Section 3 -- TREC Guidelines

 

 

Each track will have its own set of guidelines. The guidelines give a precise definition of the task(s) to be performed, a due date for the results, and any special instructions for that track.  Guidelines are generally posted on the web site and to the track mailing list in the spring.

 

Previous TRECs also had a set of guidelines for the main task.  While we have not had a main task for some time, these guidelines have become generic guidelines that all tracks assume unless explicitly stated otherwise.

 

1.  System data structures (such as dictionaries, indices, thesauri, etc. whether constructed by hand or automatically) can be built using existing documents, topics, and relevance judgments, but these structures may not be modified in response to the new test topics.  For example, you can't add topic words that are not in your dictionary, nor may the system data structures be based in any way on the results of retrieving documents for the test topics and having a human look at the retrieved documents.  Most of the tasks in TREC represent the real-world problem of an ordinary user seeking information.  If an ordinary user couldn't make the change to the system, you should not make it after receiving the test topics.  A corollary of this rule is that your system may not be tuned to the TREC 2023 topics.

 

2.  There are many possible methods for converting the supplied topics into queries that your system can execute.  TREC defines two broad categories of methods, "automatic" and "manual", based on whether manual intervention is used.  Automatic construction is when there is no human involvement of any sort in the query construction process; manual construction is everything else. Note that this is a very broad definition of manual construction, including both runs in which the queries are constructed manually and then run without looking at the results, and runs in which the results are used to alter the queries using some manual operation.

 

3.  Many tracks use the same format for submitting retrieval results.  The result of a run is generally a ranking of the top 1000 documents retrieved for each topic.  You may submit fewer than 1000 documents for a topic, but the ranked retrieval evaluation measures used in TREC evaluate to 1000 and count empty ranks as irrelevant (so your score cannot be hurt by returning 1000 documents).  Similarly, systems that do not rank documents perform poorly as evaluated by these measures.  Note that trec_eval evaluates a run based strictly on scores, NOT on the ranks you assign in your submission.  If you want the precise ranking you submit to be evaluated, the scores MUST reflect it.

 

     The format to use when submitting ranked results is as follows, using a space as the delimiter between columns.  The width of the columns in the format is not important, but it is important to include all columns and have some amount of whitespace between the columns.

 

       30 Q0 ZF08-175-870  0 4238 prise1

       30 Q0 ZF08-306-044  1 4223 prise1

       30 Q0 ZF09-477-757  2 4207 prise1

       30 Q0 ZF08-312-422  3 4194 prise1

       30 Q0 ZF08-013-262  4 4189 prise1

          etc.

    where:

       * the first column is the topic number.

 

       * the second column is the query number within that topic.  This is currently unused and should always be Q0.

 

       * the third column is the official document number of the retrieved document and is the number found in the "docno" field of the document.

 

       * the fourth column is the rank the document is retrieved, and the fifth column shows the score (integer or floating point) that generated the ranking.  This score MUST be in descending (non-increasing) order and is important to include so that tied scores (in a given run) are handled uniformly; (trec_eval sorts documents by these scores, not your ranks).

 

       * the sixth column is called the "run tag" and should be a unique identifier for your group AND for the method used. That is, each run should have a different tag that identifies  the group and the method that produced the run.  Please change the tag from year to year, since often we compare across years (for graphs and such) and having the same name show up for both years is confusing.  Also please use 12 or fewer letters and numbers, and no punctuation, to facilitate labeling graphs and such with the tags.

 

    NIST will release a routine that checks for common errors in the result files including duplicate document numbers for the same topic, invalid document numbers, wrong format, and duplicate tags across runs. This routine will be made available to participants to check their runs for errors prior to submitting them.  Submitting runs is an automatic process done through a web form, and runs that contain errors cannot be processed.
